---
title: "Stat4620_Project"
author: "Project Group 1"
date: "2024-11-20"
output: pdf_document
---

```{r}
library(ISLR)
library(pls)
library(ggplot2)
library(glmnet)
library(tidyverse)
library(broom)
library(dplyr)
library(MASS)
library(corrplot)
library(randomForest)
library(caret)
library(gridExtra)

train_data = read.csv("train.csv")
test_data = read.csv("test_new.csv")
```


#Part I: Exploratory Data Analysis

The AMES Housing data set contains information regarding to house prices and the characteristics of them. Variables ranges from numerical and categorical types of property locations, rooms and house furnishings.

```{r}
# Check missing values for each column
missing_counts <- colSums(is.na(train_data))
missing_features <- missing_counts[missing_counts > 0]
missing_features
```

There is one variable (LotFrontage) that contained a lot of actual missing values and thus we will drop it. We will also drop the ID column in the data set as it's used as an identifier and has no useful information. Upon analyzing the remaining missing features with NAs, we realized those NAs represent an actual category and are not missing data values, so we will keep them in the dataset for now.

```{r}
train_data = train_data[, !(names(train_data) %in% c("Id", "LotFrontage"))]
```

We'll also drop categorical variables that don't provide a good split of the data space. Doing this will further simplify the number of features without losing any important patterns or information. Kaggle provides us a comprehensive view of the percentage break down of the buckets in the categorical variables. We'll drop variables that have buckets that exceed 85% of the observations.

```{r}
train_data = train_data[, !(names(train_data) %in% c("Street", "Alley", "PoolQC", "MiscFeature", "LandContour", "Utilities", "LandSlope","Condition1","Condition2","RoofMatl","ExterCond","BsmtCond","BsmtFinType2","Heating","CentralAir","Electrical","Functional","GarageQual","GarageCond","PavedDrive", "SaleType"))]
```

We will then fill in the NAs for the remaining variables with missing values, replacing NAs in categorical variables with "None". There are two remaining continuous variables with missing values: GarageYrBuilt and MasVnrArea. For GarageYrBuilt, we will replace the NAs with the median value in that variable, but for "MasVnrArea", we will replace with the value 0 to correspond with the 8 missing values of categorical variable "MasVnrType". 

```{r}
summary(train_data)

missing_counts <- colSums(is.na(train_data))
missing_features <- missing_counts[missing_counts > 0]
missing_features

median_value <- median(train_data$GarageYrBlt, na.rm = TRUE)
train_data$GarageYrBlt[is.na(train_data$GarageYrBlt)] <- median_value
train_data$MasVnrArea[is.na(train_data$MasVnrArea)] <- 0

train_data[is.na(train_data)] <- "None"

colSums(is.na(train_data))#there are now no NA's
```

Now we will look at the correlation matrix of our continuous predictors and address any irrelevant features.

```{r}
# Correlation matrix for numeric features
train_data_numeric <- train_data[sapply(train_data, is.numeric)]
cor_matrix <- cor(train_data_numeric)

#subset(as.data.frame.table(cor_matrix), abs(Freq) < 1 & abs(Freq) > 0.75)

cor_sal <- cor_matrix[, "SalePrice"]
cor_sal

# All variables not highly correlated with SalePrice
names(cor_sal[abs(cor_sal) < 0.5])
```

We will remove all the continuous variables that are not highly correlated with our response variable, SalePrice, based on the correlation matrix above. Those continuous variables with a correlation value higher than 0.5 or lower than -0.5 will remain in our dataset.

```{r}
train_data = train_data[, !(names(train_data) %in% c("MSSubClass", "LotArea", "OverallCond", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "X2ndFlrSF", "LowQualFinSF", "BsmtFullBath", "BsmtHalfBath", "HalfBath", "BedroomAbvGr", "KitchenAbvGr", "Fireplaces", "WoodDeckSF", "OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "PoolArea", "MiscVal", "MoSold", "YrSold"))]
```

We will now look at the correlation between all predictor variables to see if there are any two that are highly correlated. If two of them are highly correlated then we will remove the one that is least correlated with the response variable.

```{r}
# Create correlation plot
corrplot(cor_matrix, method = "color", tl.cex = 0.5)

# Print all relationships with 0.75 correlation or more
subset(as.data.frame.table(cor_matrix), abs(Freq) < 1 & abs(Freq) > 0.75)
```

In the table above we can see that 4 of the predictor variables are highly correlated with another 4 variables so we will remove those, keeping the ones with higher correlation to the response.

```{r}
# Remove variables due to multicollinearity
train_data = train_data[, !(names(train_data) %in% c("GarageYrBlt", "X1stFlrSF", "TotRmsAbvGrd", "GarageArea"))]
```

We will now look at all the categorical variables to see if they all have a unique distribution of SalePrice across different categories, deeming them useful.

```{r warning=FALSE}
# List of all categorical variables
categorical_vars <- c("MSZoning", "LotShape", "LotConfig", "Neighborhood", "BldgType", "HouseStyle", "RoofStyle", "Exterior1st", "Exterior2nd", "MasVnrType", "ExterQual", "Foundation","BsmtQual","BsmtExposure","BsmtFinType1","HeatingQC", "KitchenQual","FireplaceQu", "GarageType","GarageFinish", "Fence", "SaleCondition")

plot_list <- list()

# Loop through categorical variables and store plots in the list
for (var in categorical_vars) {
  x <- ggplot(train_data, aes_string(x = var, y = "SalePrice")) +
    geom_bar(stat = "summary", fun = "mean", fill = "steelblue") +
    labs(title = paste("SalePrice by", var),
         x = var, y = "Average SalePrice") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

  # Add the plot to the list
  plot_list[[length(plot_list) + 1]] <- x
}

# Arrange and print the plots two at a time
for (i in seq(1, length(plot_list), by = 2)) {
  plots_to_print <- plot_list[i:min(i + 1, length(plot_list))]
  grid.arrange(grobs = plots_to_print, ncol = 2)
}
```

We can see that for each categorical variable that the SalePrice is different across each category in each categorical variable which is good and tells us that they will all be useful. 

#Part II: Model Analysis

+After cleaning our data and performing EDA we are going to fit a regression tree model to our data.

+A regression tree model is a decision tree that predicts a continuous variable. It predicts by recursively partitioning the predictor space into smaller and smaller subregions the more we split the tree. The tree defines local regions using a step-function-like approach.

+The regression tree model would give us the best results because they work with complex data sets where there's a mix of categorical and numerical variables.Trees also don't have the typical linear regression interpretation, so we wouldn't need to create indicator variables to represent the categorical features. Lastly, regression trees allow us to easily interpret the results.

+The regression tree model makes minimal assumptions on the relationships in the data set. The assumption generally being that the data can be partitioned into subsets and that each split is independent and interpretable. 

```{r}
cv <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation
mtry_grid <- expand.grid(.mtry = c(15, 20, 25, 30)) # Tuning grid for mtry

# Train the model using random forest with cross-validation
set.seed(123)
rf_cv_model <- train(SalePrice ~ .,data = train_data,method = "rf",trControl = cv,tuneGrid = mtry_grid,ntree = 500)
print(rf_cv_model)
```

After running cross-validation to select the best value for mtry which represents the number of predictors sampled for splitting at each node. We can see that mtry=30 gives the best results. We will now fit the model with 500 trees and mtry=30

```{r}
set.seed(123)
rf_model <- randomForest(SalePrice ~ .,data = train_data,ntree = 500, mtry = 30, importance = TRUE)
print(rf_model)
```

After fitting the model we got an R^2 value of 0.8218, indicating that 82% of the variance in the response variable is explained by the selected features. This suggests that the model provided a strong fit for the data.

```{r}
par(cex = 0.7)
varImpPlot(rf_model, type = 1) # Plot variable importance
```

We can see that the most important variable is `GrLiveArea` (Above ground living area square feet) which makes sense because larger houses will cost more. Along with that variable, `OverallQual` (Overall material and finish of the house) and `TotalBsmtSF` (Total square feet of the basement) are also very important to the model and separate themselves from the other variables. `TotalBsmtSF`is very similar to `GrLiveArea` and probably gives a similar value so that explains why it is so important and if the quality of the house is low then the price will also be lower. 

We will now run the test data through the pre-processing and then evaluate it's performance with the model.

```{r}
#Pre-Processing on test_data
test_data = test_data[, !(names(test_data) %in% c("Id", "LotFrontage", "Street", "Alley", "PoolQC", "MiscFeature", "LandContour", "Utilities", "LandSlope","Condition1","Condition2","RoofMatl","ExterCond","BsmtCond","BsmtFinType2","Heating","CentralAir","Electrical","Functional","GarageQual","GarageCond","PavedDrive", "SaleType", "GarageYrBlt", "X1stFlrSF", "TotRmsAbvGrd", "GarageArea", "MSSubClass", "LotArea", "OverallCond", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "X2ndFlrSF", "LowQualFinSF", "BsmtFullBath", "BsmtHalfBath", "HalfBath", "BedroomAbvGr", "KitchenAbvGr", "Fireplaces", "WoodDeckSF", "OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "PoolArea", "MiscVal", "MoSold", "YrSold"))]

test_data$MasVnrArea[is.na(test_data$MasVnrArea)] <- 0
test_data[is.na(test_data)] <- "None"
```

```{r}
test_x = test_data[, !(names(test_data) == "SalePrice")] #predictors of test data
test_y = test_data[, (names(test_data) == "SalePrice")] #response of test data

predictions <- predict(rf_model, newdata = test_x) #predict sale price on test data
```

```{r}
#data drame of results
results = data.frame(Actual = test_y, Predicted = predictions)

# Scatterplot of actual vs predicted values
ggplot(data = results, aes(x = Actual, y = Predicted)) +
  geom_point(color = "blue", alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "solid") +
  labs(title = "Actual vs Predicted Prices",
       x = "Actual Price",
       y = "Predicted Price") +
  theme_minimal()
```

```{r}

rmse <- sqrt(mean((predictions - test_y)^2)) #get rmse of predictions
cat("RMSE: ", rmse, "\n")
```

After running the test data through the model we can see that on average we are $26,306.08 off from the actual sale price.
